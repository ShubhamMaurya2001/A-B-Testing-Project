# ğŸ® Cookie Cats A/B Testing â€“ Decisionâ€‘Focused Experimentation (Industry Grade)

## ğŸ“Œ Project Overview

This project presents an **endâ€‘toâ€‘end, industryâ€‘style A/B testing analysis** conducted on the *Cookie Cats* mobile game dataset (~90,000 users). The goal was to evaluate whether moving a progression gate from **Level 30 (Control)** to **Level 40 (Treatment)** improves engagement **without harming user retention**.

Unlike typical notebookâ€‘only analyses, this project mirrors how **product data science teams** operate:

* Experiment sanity checks (SRM)
* Robust statistical inference
* Power validation
* Business impact estimation
* Clear ship / noâ€‘ship recommendation

---

## ğŸ§  Business Context

In freeâ€‘toâ€‘play mobile games, **early retention directly drives lifetime value (LTV)**. Progression gates are often introduced to increase engagement, but excessive friction can lead to churn.

**Business Question:**

> Should the progression gate be moved from Level 30 to Level 40?

**Primary Metric (North Star):**

* **7â€‘day retention** (proxy for longâ€‘term value)

**Secondary / Guardrail Metrics:**

* 1â€‘day retention
* Engagement (number of game rounds)

---

## ğŸ§ª Experiment Design

* **Control:** Gate at Level 30 (`gate_30`)
* **Treatment:** Gate at Level 40 (`gate_40`)
* **Users:** ~90,189 players
* **Split:** Approximately 50/50 randomized allocation

---

## ğŸ” Analysis Workflow & Results

### 1ï¸âƒ£ Sanity Check â€“ Sample Ratio Mismatch (SRM)

Before analyzing outcomes, experiment integrity was validated using a **chiâ€‘square SRM test**.

**Observed allocation:**

* gate_30: 44,700 users
* gate_40: 45,489 users

**SRM Test Result:**

* pâ€‘value = **0.0086** (statistically detectable imbalance)

**Interpretation:**
With a large sample size (~90k users), small allocation deviations (~1%) can become statistically detectable. The imbalance magnitude was operationally small and did not materially affect directional conclusions.

To ensure robustness, results were validated using:

* Bootstrapâ€‘based uncertainty estimation
* Power analysis confirming sufficient sensitivity
* Consistent effect direction across metrics

ğŸ“Œ **Conclusion:** Results are treated as **directionally reliable**, with a recommendation to audit the experimentation pipeline before rerunning.

---

### 2ï¸âƒ£ Data Cleaning & Exploration

* Engagement (`sum_gamerounds`) is **highly rightâ€‘skewed**
* One extreme outlier (~49k rounds) heavily distorts averages

**Action Taken:**

* Removed the extreme outlier to enable robust comparisons
* Used logâ€‘scaled visualizations for interpretability

---

### 3ï¸âƒ£ Retention Analysis (Bootstrap Inference)

Retention metrics are binary and influenced by skewed engagement behavior.

**Method:**

* Nonâ€‘parametric bootstrap (1,000 iterations)
* Estimated distribution of Treatment âˆ’ Control differences

#### ğŸ“Š Results

**1â€‘Day Retention**

* gate_30: **44.82%**
* gate_40: **44.23%**
* Probability treatment is worse: **95.3%**

**7â€‘Day Retention (Primary Metric)**

* gate_30: **19.02%**
* gate_40: **18.20%**
* Absolute difference: **âˆ’0.82%**
* Probability treatment is worse: **99.9%**

ğŸ“Œ Interpretation: Gate 40 shows a **clear and highly confident negative impact** on longâ€‘term retention.

---

### 4ï¸âƒ£ Engagement Analysis (Mannâ€“Whitney U Test)

Engagement is heavyâ€‘tailed and nonâ€‘normal.

**Method:** Mannâ€“Whitney U test (nonâ€‘parametric)

**Results:**

* Avg rounds (gate_30): 51.34
* Avg rounds (gate_40): 51.30
* pâ€‘value: 0.0509

ğŸ“Œ Interpretation: **No meaningful engagement improvement**, despite increased friction.

---

## ğŸ“Š Experiment Power Analysis

To validate experimental sensitivity, a power analysis was conducted.

**Assumptions:**

* Baseline 7â€‘day retention â‰ˆ 18%
* Minimum Detectable Effect (MDE): 1% absolute
* Power: 80%, Î± = 0.05

**Result:**

* Required sample per group: ~23,664
* Actual sample per group: ~45,094

âœ… **Conclusion:** Experiment was sufficiently powered to detect businessâ€‘relevant effects.

---

## ğŸ’° Business Impact Simulation

Statistical significance alone is insufficient for decisionâ€‘making.

**Illustrative Assumptions:**

* Each 7â€‘day retained user â‰ˆ â‚¹500 lifetime value

**Estimated Impact:**

* ~0.82% absolute drop in 7â€‘day retention
* ~818 fewer retained users per 100k players
* **Estimated revenue risk:** ~â‚¹4.1 lakh per 100k users

ğŸ“Œ Even small retention drops translate into **material longâ€‘term revenue loss**.

---

## ğŸ” Segmented Analysis

Users were segmented by engagement level (above vs below median rounds).

**Findings:**

* Absolute retention drop is larger among **highâ€‘engagement users**
* Lowâ€‘engagement users remain fragile overall, compounding churn risk

ğŸ“Œ Insight: Average effects hide risk; segmentation reveals who is most impacted.

---

## ğŸ§¾ Final Decision Memo

**Primary Metric:** 7â€‘day retention
**Guardrail Metric:** Engagement

* Retention â†“ (statistically & practically significant)
* Engagement â‰ˆ flat
* Downside risk outweighs any potential upside

### âœ… Final Recommendation

**DO NOT DEPLOY** â€” retain the progression gate at **Level 30**.

---

## ğŸš§ Limitations

* Observational analysis on historical experiment data
* Revenue values are simulated for illustration
* No longâ€‘term cohort or monetization breakdown

---

## ğŸš€ Future Work

* Audit and rerun experiment to address SRM
* Targeted gating experiments for highâ€‘engagement users

---

## ğŸ§  Key Skills Demonstrated

* Experiment design & validation (SRM, power analysis)
* Nonâ€‘parametric statistical inference
* Bootstrap uncertainty estimation
* Segmentation & decision analysis
* Translating metrics into business impact

